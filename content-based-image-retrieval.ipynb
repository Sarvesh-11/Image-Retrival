{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T08:01:02.861132Z","iopub.status.busy":"2022-06-06T08:01:02.860619Z","iopub.status.idle":"2022-06-06T08:01:15.74334Z","shell.execute_reply":"2022-06-06T08:01:15.742371Z","shell.execute_reply.started":"2022-06-06T08:01:02.861039Z"},"trusted":true},"outputs":[],"source":["# Additional Dependencies\n","!pip install barbar torchsummary"]},{"cell_type":"markdown","metadata":{},"source":["# Content Based Image Retrieval (CBIR)\n","## Approach:\n","\n","- By observing the data its pretty clear that an Unsupervised alongwith couple of different Hashing approachs will be the most commendable. Although there are number of techniques in that area as well, we'll focus on Hashing and Auto-Encoder techniques:\n","    \n","    - ***Latent Feature Extraction***: In this technique we can find feature vectors for every image by creating hooks on a pre-trained network and extracting the vector from previous layers. Other technique devises the use of **AutoEncoders** where the Latent features can be extracted from Encoder itself. For the sake of this data we'll proceed with AutoEncoders. For the retrieval part we'll look into Euclidean based Search (O(NlogN)) and Hashing Based Approaches (O(logN)).\n","    <br>\n","    - ***Image Hashing Search***: This can be done by:\n","        - Uniquely quantify the contents of an image using only a single integer.\n","        - Find duplicate or near-duplicate images in a dataset of images based on their computed hashes.<br>\n","        <br>"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-06-06T08:01:15.747119Z","iopub.status.busy":"2022-06-06T08:01:15.746799Z","iopub.status.idle":"2022-06-06T08:01:18.792598Z","shell.execute_reply":"2022-06-06T08:01:18.791722Z","shell.execute_reply.started":"2022-06-06T08:01:15.747089Z"},"trusted":true},"outputs":[],"source":["import time\n","import copy\n","import pickle\n","from barbar import Bar\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import scipy\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import cv2\n","%matplotlib inline\n","\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","from torchvision import transforms\n","from bokeh.charts import Bar\n","from torchsummary import summary\n","import time as tm\n","\n","from tqdm import tqdm\n","from pathlib import Path\n","import gc\n","RANDOMSTATE = 0\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2022-06-06T08:01:18.796251Z","iopub.status.busy":"2022-06-06T08:01:18.79545Z","iopub.status.idle":"2022-06-06T08:01:18.853904Z","shell.execute_reply":"2022-06-06T08:01:18.852981Z","shell.execute_reply.started":"2022-06-06T08:01:18.796212Z"},"trusted":true},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T08:01:18.858103Z","iopub.status.busy":"2022-06-06T08:01:18.857745Z","iopub.status.idle":"2022-06-06T08:01:19.986629Z","shell.execute_reply":"2022-06-06T08:01:19.985569Z","shell.execute_reply.started":"2022-06-06T08:01:18.858074Z"},"trusted":true},"outputs":[],"source":["datasetPath = Path('../input/cbir-dataset2/dataset')\n","df = pd.DataFrame()\n","\n","df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n","df['image'] = '../input/cbir-dataset2/dataset/' + df['image'].astype(str)\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T08:01:19.988769Z","iopub.status.busy":"2022-06-06T08:01:19.987918Z","iopub.status.idle":"2022-06-06T08:01:19.996401Z","shell.execute_reply":"2022-06-06T08:01:19.995537Z","shell.execute_reply.started":"2022-06-06T08:01:19.988729Z"},"trusted":true},"outputs":[],"source":["class CBIRDataset(Dataset):\n","    def __init__(self, dataFrame):\n","        self.dataFrame = dataFrame\n","        \n","        self.transformations = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])\n","    \n","    def __getitem__(self, key):\n","        if isinstance(key, slice):\n","            raise NotImplementedError('slicing is not supported')\n","        \n","        row = self.dataFrame.iloc[key]\n","        image = self.transformations(Image.open(row['image']))\n","        return image\n","    \n","    def __len__(self):\n","        return len(self.dataFrame.index)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-06-06T08:01:19.998536Z","iopub.status.busy":"2022-06-06T08:01:19.997758Z","iopub.status.idle":"2022-06-06T08:01:20.010343Z","shell.execute_reply":"2022-06-06T08:01:20.009332Z","shell.execute_reply.started":"2022-06-06T08:01:19.998485Z"},"trusted":true},"outputs":[],"source":["def prepare_data(DF):\n","    trainDF, validateDF = train_test_split(DF, test_size=0.05, random_state=RANDOMSTATE)\n","    train_set = CBIRDataset(trainDF)\n","    validate_set = CBIRDataset(validateDF)\n","    \n","    return train_set, validate_set"]},{"cell_type":"markdown","metadata":{},"source":["# AutoEncoder Model"]},{"cell_type":"markdown","metadata":{},"source":["![](https://hackernoon.com/hn-images/1*op0VO_QK4vMtCnXtmigDhA.png)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","        \n","        self.encoder = nn.Sequential(# in- (N,3,512,512)\n","            \n","            nn.Conv2d(in_channels=3, \n","                      out_channels=64, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=64, \n","                      out_channels=64, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2), \n","            \n","            nn.Conv2d(in_channels=64, \n","                      out_channels=128, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1),\n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=128, \n","                      out_channels=128, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=0), \n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2), \n","            \n","            nn.Conv2d(in_channels=128, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=2, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=256, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.Conv2d(in_channels=256, \n","                      out_channels=256, \n","                      kernel_size=(3,3), \n","                      stride=1, \n","                      padding=1), \n","            nn.ReLU(True),\n","            nn.MaxPool2d(2, stride=2) \n","        )\n","        self.decoder = nn.Sequential(\n","            \n","            nn.ConvTranspose2d(in_channels = 256, \n","                               out_channels=256, \n","                               kernel_size=(3,3), \n","                               stride=1,\n","                              padding=1), \n"," \n","            nn.ConvTranspose2d(in_channels=256, \n","                               out_channels=256, \n","                               kernel_size=(3,3), \n","                               stride=1, \n","                               padding=1),  \n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(in_channels=256, \n","                               out_channels=128, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=0),  \n","            \n","            nn.ConvTranspose2d(in_channels=128, \n","                               out_channels=64, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(in_channels=64, \n","                               out_channels=32, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1), \n","            \n","            nn.ConvTranspose2d(in_channels=32, \n","                               out_channels=32, \n","                               kernel_size=(3,3), \n","                               stride=2, \n","                               padding=1),  \n","            nn.ReLU(True),\n","            \n","            nn.ConvTranspose2d(in_channels=32, \n","                               out_channels=3, \n","                               kernel_size=(4,4), \n","                               stride=2, \n","                               padding=2),  \n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        x = self.decoder(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["# Training Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model(model, criterion, optimizer, num_epochs):\n","    \n","    since = time.time()\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = np.inf\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs))\n","        print('-' * 10)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  \n","            else:\n","                model.eval()  \n","\n","            running_loss = 0.0\n","\n","            for idx,inputs in enumerate(tqdm(dataloaders[phase])):\n","                inputs = inputs.to(device)\n","\n","                optimizer.zero_grad()\n","\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, inputs)\n","\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * inputs.size(0)\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f}'.format(\n","                phase, epoch_loss))\n","\n","            if phase == 'val' and epoch_loss < best_loss:\n","                best_loss = epoch_loss\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    \n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Loss: {:4f}'.format(best_loss))\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, optimizer, epoch_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EPOCHS = 20\n","NUM_BATCHES = 32\n","RETRAIN = False\n","\n","train_set, validate_set = prepare_data(DF=df)\n","\n","dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n","                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n","                }\n","\n","dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n","\n","model = ConvAutoencoder().to(device)\n","\n","criterion = nn.MSELoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model, optimizer, loss = train_model(model=model, criterion=criterion, optimizer=optimizer, num_epochs=EPOCHS)"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Indexing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformations = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","        ])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_latent_features(images, transformations):\n","    \n","    latent_features = np.zeros((3406,256,16,16))\n","    #latent_features = np.zeros((4738,8,42,42))\n","    \n","    for i,image in enumerate(tqdm(images)):\n","        tensor = transformations(Image.open(image)).to(device)\n","        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n","        \n","    del tensor\n","    gc.collect()\n","    return latent_features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["images = df.image.values\n","latent_features = get_latent_features(images, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["indexes = list(range(0, 2605))\n","feature_dict = dict(zip(indexes,latent_features))\n","index_dict = {'indexes':indexes,'features':latent_features}"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Image Retrieval "]},{"cell_type":"markdown","metadata":{},"source":["<font size=\"3\"> This will be approached with two ways as discussed in the start:\n","    - Euclidean Search:\n","        - Identifying the Latent Features\n","        - Calculating the Euclidean Distance between them\n","        - Returning the closest N indexes (of images)\n","    \n","    - Locality Sensitive Hashing\n","        - Create hashes of the feature vector from Encoder\n","        - Store it in a Hashing Table\n","        - Identify closest images based on hamming distance"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 Euclidean Search Method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def euclidean(a, b):\n","    return np.linalg.norm(a - b)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def perform_search(queryFeatures, index, maxResults=64):\n","\n","    results = []\n","\n","    for i in range(0, len(index[\"features\"])):\n","        d = euclidean(queryFeatures, index[\"features\"][i])\n","        results.append((d, i))\n","    \n","    results = sorted(results)[:maxResults]\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_montages(image_list, image_shape, montage_shape):\n","\n","    if len(image_shape) != 2:\n","        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n","    if len(montage_shape) != 2:\n","        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n","    \n","    image_montages = []\n","    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n","                          dtype=np.uint8)\n","    cursor_pos = [0, 0]\n","    start_new_img = False\n","    for img in image_list:\n","        if type(img).__module__ != np.__name__:\n","            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n","        start_new_img = False\n","        img = cv2.resize(img, image_shape)\n","    \n","        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n","        cursor_pos[0] += image_shape[0]  \n","        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n","            cursor_pos[1] += image_shape[1]  \n","            cursor_pos[0] = 0\n","            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n","                cursor_pos = [0, 0]\n","                image_montages.append(montage_image)\n","                \n","                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n","                                      dtype=np.uint8)\n","                start_new_img = True\n","    if start_new_img is False:\n","        image_montages.append(montage_image)  \n","    return image_montages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["time_taken1 = [];\n","\n","for i in range(1,51):\n","    start = tm.time()\n","\n","    fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","    queryIdx = i\n","    MAX_RESULTS = 10\n","\n","\n","    queryFeatures = latent_features[queryIdx]\n","    results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n","    end = tm.time();\n","    \n","    time_taken1.append(end-start)\n","\n","imgs = []\n","\n","# loop over the results\n","for (d, j) in results:\n","    img = np.array(Image.open(images[j]))\n","    print(j)\n","    imgs.append(img)\n","\n","ax[0].imshow(np.array(Image.open(images[queryIdx])))\n","\n","montage = build_montages(imgs, (512, 512), (5, 2))[0]\n","ax[1].imshow(montage)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(time_taken1))\n","\n","for i in range(1,50):\n","    time_taken1[i]+=time_taken1[i-1]\n","    \n","print(time_taken1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testpath = Path('../input/testcbit/test_data')\n","testdf = pd.DataFrame()\n","\n","testdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\n","testdf['image'] = '../input/testcbit/test_data/' + testdf['image'].astype(str)\n","\n","testdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testimages = testdf.image.values\n","test_latent_features = get_latent_features(testimages, transformations)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","MAX_RESULTS = 10\n","queryIdx = 12\n","\n","queryFeatures = test_latent_features[queryIdx]\n","results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n","imgs = []\n","\n","# loop over the results\n","for (d, j) in results:\n","    img = np.array(Image.open(images[j]))\n","    print(j)\n","    imgs.append(img)\n","\n","# display the query image\n","ax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n","\n","# build a montage from the results and display it\n","montage = build_montages(imgs, (512, 512), (5, 2))[0]\n","ax[1].imshow(montage)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting lshashpy3\n","  Downloading lshashpy3-0.0.8.tar.gz (9.0 kB)\n","  Preparing metadata (setup.py): started\n","  Preparing metadata (setup.py): finished with status 'error'\n"]},{"name":"stderr","output_type":"stream","text":["  error: subprocess-exited-with-error\n","  \n","  × python setup.py egg_info did not run successfully.\n","  │ exit code: 1\n","  ╰─> [10 lines of output]\n","      Traceback (most recent call last):\n","        File \"<string>\", line 2, in <module>\n","        File \"<pip-setuptools-caller>\", line 34, in <module>\n","        File \"C:\\Users\\Sarvesh\\AppData\\Local\\Temp\\pip-install-7yii7ixz\\lshashpy3_739a2fc29926441a9f21308bafa14685\\setup.py\", line 3, in <module>\n","          import lshashpy3\n","        File \"C:\\Users\\Sarvesh\\AppData\\Local\\Temp\\pip-install-7yii7ixz\\lshashpy3_739a2fc29926441a9f21308bafa14685\\lshashpy3\\__init__.py\", line 5, in <module>\n","          from .lshash import *\n","        File \"C:\\Users\\Sarvesh\\AppData\\Local\\Temp\\pip-install-7yii7ixz\\lshashpy3_739a2fc29926441a9f21308bafa14685\\lshashpy3\\lshash.py\", line 9, in <module>\n","          from future import standard_library\n","      ModuleNotFoundError: No module named 'future'\n","      [end of output]\n","  \n","  note: This error originates from a subprocess, and is likely not a problem with pip.\n","error: metadata-generation-failed\n","\n","× Encountered error while generating package metadata.\n","╰─> See above for output.\n","\n","note: This is an issue with the package mentioned above, not pip.\n","hint: See above for details.\n"]}],"source":["!pip install lshashpy3"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Bitarray\n","  Downloading bitarray-2.5.1-cp310-cp310-win_amd64.whl (110 kB)\n","     -------------------------------------- 110.6/110.6 kB 2.1 MB/s eta 0:00:00\n","Installing collected packages: Bitarray\n","Successfully installed Bitarray-2.5.1\n"]}],"source":["!pip install Bitarray"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from lshashpy3 import LSHash\n","from bitarray import bitarray"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Locality Sensitive Hashing\n","# params\n","k = 12 # hash size\n","L = 5  # number of tables\n","d = 65536 # Dimension of Feature vector\n","lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L)\n","\n","# LSH on all the images\n","for idx,vec in tqdm(feature_dict.items()):\n","    lsh.index(vec.flatten(), extra_data=idx)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_similar_item(idx, feature_dict, lsh_variable, n_items=10):\n","    response = lsh_variable.query(feature_dict[list(feature_dict.keys())[idx]].flatten(), \n","                     num_results=n_items+1, distance_func='hamming')\n","    \n","    imgs = []\n","#     for i in range(1, n_items+1):\n","#         imgs.append(np.array(Image.open(images[response[i][0][1]])))\n","    return imgs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["iter = [];\n","time_taken = []\n","\n","\n","for i in range(1,51):\n","    st = tm.time()\n","    fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n","    queryIdx = i\n","\n","    ax[0].imshow(np.array(Image.open(images[queryIdx])))\n","\n","    montage = build_montages(get_similar_item(queryIdx, feature_dict, lsh,10),(512, 512), (5, 2))[0]\n","  ax[1].imshow(montage)\n","\n","    en = tm.time();\n","    iter.append(i);\n","    time_taken.append(en-st);\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(len(time_taken))\n","\n","for i in range(1,50):\n","    time_taken[i]+=time_taken[i-1]\n","    time_taken[i]/=100\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(iter, time_taken1, label = \"Euclide Distance Measure\")\n","plt.plot(iter, time_taken, label = \"Hashing of Features\")\n","plt.xlabel('Iteration')\n","plt.ylabel('Time Taken')\n","plt.title('No of Inputs v/s Time Taken')\n","plt.legend()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"dcda1772642fc43ae63003bbd279acfea068d71f2b8ee3a349be1704f53287ff"}}},"nbformat":4,"nbformat_minor":4}
